submissions:

622: 
SGDClassifier(loss='log',penalty='l2',alpha=0.0001,n_iter=30,shuffle=True,random_state=1337) 
take top 42% of trigram features
CV: 0.889389

628:
Columns:
[0, 7, 9, 10, 11, 32, 37, 42, 43, 48, 57, 60, 63, 64, 65, 66, 67, 68, 70, 72, 80, 83, 86, 89]
Logistic Regression C = 2.3, class_weight='auto'
CV: 0.907566

629:
Columns:
[70, 92, 9, 53, 89, 40, 67, 7, 62, 11, 32, 65, 48, 19, 37, 86, 0, 68]
Logistic Regression C = 2.3, class_weight='auto'
CV: 0.907908


630Quad Cut at 0.5:
Columns:
[105, 66, 32, 10, 138, 99, 141, 42, 124, 34, 143, 103, 107, 144, 11, 7, 59, 161, 48, 19, 37, 158, 0, 139]
Logistic Regression C = 2.3, class_weight='auto'
CV: 0.909219 (10Fold), 10 iteration 20%: 0.9056306
Submitted: 0.91607
Blend with 622 -> 0.91693
Training Blend -> results are very uncorrelated but simple blend doesnt boost
CV: 0.909110 (Need a better way to estimate boost?)

Quad Cut at 0.25
[67, 82, 130, 143, 32, 10, 60, 42, 98, 162, 48, 68, 128, 93, 86, 65, 11, 7, 64, 120, 69, 34, 84, 37, 70, 0]
CV: 0.909655 (10Fold), 10 iteration 20%: 0.9062426

701Trips Variable cuts: tripsFractions.csv
[98,336,294,19,205,290,226,211,244,38,9,208,18,35,148,295,341,262,12,210, 233, 338, 0, 320]
CV: 0.9104 (10Fold), 10 iteration 20%: 0.9076
Blended with 622: 0.91920

704Quads Variable cuts
base = [183, 266, 211, 178, 70, 363, 329, 248, 331,327, 368,365,66,184, 7, 261, 234, 47, 322, 216,310,177,127,9,38,18,262,313,343,180,35,362,267,174,254,12,182,205,382,0,292]
CV: 0.9108 (10 fold), 10 iteration 20%: 0.9082
Blended with 622: 0.91954, by itself 0.91962

707Trips Cut at increments of 0.1
base = [8, 315, 344, 293, 798, 310, 739, 547, 511, 794,105,500, 709, 122, 74, 7, 362, 28, 596, 737,845, 546, 748, 0, 706, 618, 37, 799, 600]
CV: 0.911

708Quads (took best from Trips and added Quads at increments of 0.1)
bestFeatures = [429, 663,427,308,594, 13, 22, 279, 107, 360, 15, 557, 16, 583, 431, 23, 19, 724, 27, 425]
CV: 0.912560 (10 fold), 10 iteration 20%: 0.9092
blended with 704: CV:0.9130 -> leaderboard: 0.91936
bended with 704 and 707: CV:0.91315 -> leaderboard: 0.91905

709
base = [3, 4, 17, 10, 123, 129, 292, 32, 9, 2, 7, 76, 5, 429, 663, 427, 308, 594, 13, 22, 279, 107, 360, 15, 557, 16, 583, 431, 23, 19, 724, 27, 425]

714
gbmQTrain
rfTrain
naive bayes
logistic regression

718: redid 701 tripsFractions.csv
base = [98,336,294,19,205,290,226,211,244,38,9,18,35,148,295,341,262,12,210, 233, 0, 320]
cv: 0.91000, leaderboard: 0.91944

720 blend
logistic regression, tripsFractions, C = 2.3
	base = [98,336,294,19,205,290,226,211,244,38,9,18,35,148,295,341,262,12,210, 233, 0, 320] # seed 1337
	base = [212,94, 47, 291, 81, 121, 205, 204, 295, 138, 7, 258, 210, 234, 282, 0, 320] # seed 918
	base = [313, 291, 151, 64, 67, 20, 290, 112, 155, 138, 18, 285, 66, 212, 233, 204, 7, 208, 68, 282, 0, 210, 9, 295, 317] # seed 622
	base = [201, 294, 260, 67, 220, 235, 7, 176, 290, 48, 309, 156, 66, 263, 138, 262, 35, 18, 233, 208, 240, 338, 0, 210, 9, 295, 317] # seed 410

sgd
params = {'loss':'log','penalty':'l2','alpha':0.0001,'n_iter':30,
		'shuffle':True,'random_state':1337,'class_weight':None}
base = [289, 332, 201, 260, 235, 240, 38, 48, 18, 212, 63, 12, 205, 263, 65, 262, 0, 338, 122, 300, 98, 210, 295, 320] # seed 410

naive_bayes, tripsFractions, alpha=1e-3
base = [289, 332, 201, 260, 235, 240, 38, 48, 18, 212, 63, 12, 205, 263, 65, 262, 0, 338, 122, 300, 98, 210, 295, 320]